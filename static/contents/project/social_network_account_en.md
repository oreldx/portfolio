
After [my automated YouTube channel project](/project/auto_youtube_channel), I spent quite some time thinking about trying a similar experiment on another platform. With social media like Instagram and TikTok trending, I decided to start a project to automate multiple accounts.

## Introduction

When designing these accounts, I didn’t want to rely on existing media content — I wanted to **create something new and original**. With the rise of **generative AI**, this felt like the perfect opportunity to explore that approach.

Since I already had experience with code-based video editing, I knew the real challenge would be **creating the content itself** and improving my **video editing workflow**.

## Goal

My goal was to **create accounts with their own unique identities**. While the structure of each account would remain fairly similar, each one would have its own personality and branding.

To begin, I decided to focus on the technical aspects and chose the simplest possible concept: _a poetry account_.  
The idea was to publish **a daily poetry reading**, with a _pleasant video and audio background, accompanied by the poem’s text scrolling on screen_.

## Content Generation

I split content generation into two main parts: **text script creation** and **resource production** for the video.

For the first part, I used the most popular LLM at the time, OpenAI’s **GPT-3.5**. With some **prompt engineering**, I achieved a consistent and reliable output for my video scripts.

For the second part, I used **Pexels** to source royalty-free background videos. The LLM would select the ideal background during the initial prompt (by choosing a name **from** a predefined list). This was essentially a form of _Retrieval-Augmented Generation_ (**RAG**), but without embeddings or vector storage — unnecessary for the small dataset involved.

For the poem’s voiceover, I used the leading provider in the field, **ElevenLabs**, whose API produces high-quality synthetic voices.  
I generated the audio directly from the text provided by the LLM.

One missing element remained: **subtitles**. I had the text and voice, but they weren’t synchronized yet. This **text alignment** step was handled by an open-source model that injected my raw text into the audio timeline, avoiding transcription errors that a tool like Whisper might have introduced.

Later, I discovered that ElevenLabs offered a dedicated API endpoint for direct text-to-voice alignment, which simplified the process further.

## Video Creation

In my previous project, I used **MoviePy** and **FFmpeg** for video editing. This time, I decided to rely exclusively on **FFmpeg**. Drawing inspiration from tools I was familiar with, such as _Adobe Premiere Pro_, I built **my own Python-based video editing SDK**.

By adopting the concepts of **layers** and **filters**, I developed an editor capable of handling images, videos, and audio tracks.  
This allowed me to create dynamic subtitle animations.

> One limitation of this approach was the difficulty of directly manipulating text.  
> To work around this, I used the **Pillow** library to generate images for each state of the text, which I then integrated into the video using FFmpeg.

<div align="center"> <video src="/assets/social_network_account/video.mp4" loop controls style="max-height: 50vh; padding: 0 2rem;"></video> </div> **Example Video**

## Publishing

For publishing, I used Meta’s API for Instagram. However, it required the video and thumbnail to be hosted on a server — it was not possible to upload the file directly from the client. This added some extra work to the **publishing workflow**.

## Multi-Account Management

Once the first part of the project was completed, I wanted to extend the **solution to multiple accounts**. To achieve this, I **refactored** a significant portion of the codebase so the same tools could be reused for different accounts, while maintaining development flexibility.

To prepare for a potential increase in the number of accounts — which would require scaling the hosting infrastructure — I implemented a **queuing architecture** using **Redis** and **Celery**. This allowed content generation and publishing tasks to be handled **asynchronously**, ensuring **horizontal scalability**.

> In practice, I always used a single queue with one worker for video rendering, as the hosting infrastructure did not have the resources to run multiple rendering instances in parallel.

To make managing multiple accounts easier, I also developed a frontend interface with **Vue.js**. It provided **real-time visibility** into the execution of each step for every account and allowed viewing the publishing history.

## Conclusion

After running the project for **several months**, I eventually decided to shut it down. The few accounts I created quickly became **lost in the sea of content** generated by other users. I explored and started developing **more original account concepts**, but Instagram’s unclear (and sometimes **unjustifiably restrictive**) policies on managing multiple accounts led me to abandon the idea.  
Since TikTok’s API access is nearly impossible, I was unable to move the project to that platform.

This project helped me gain a deeper understanding of **separation of concerns in code logic** and managing **scalability** through **asynchronous processing**.  
It also made me aware of the limitations of my current integrations — particularly in video editing — which led me to explore promising external solutions such as **Remotion**.
